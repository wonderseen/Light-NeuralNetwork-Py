nohup: ignoring input
trunc_gaussion_initializer for a tensor of shape [8, 5].
trunc_gaussion_initializer for a tensor of shape [8, 8].
trunc_gaussion_initializer for a tensor of shape [1, 8].
step=0 test_loss=0.3965 lr=0.1000 label_mean=0.3922
step=2000 test_loss=0.0731 lr=0.0990 label_mean=0.3922
step=4000 test_loss=0.0727 lr=0.0965 label_mean=0.3922
step=6000 test_loss=0.0723 lr=0.0935 label_mean=0.3922
step=8000 test_loss=0.0720 lr=0.0910 label_mean=0.3922
step=10000 test_loss=0.0717 lr=0.0900 label_mean=0.3922
step=12000 test_loss=0.0713 lr=0.0900 label_mean=0.3922
step=14000 test_loss=0.0709 lr=0.0900 label_mean=0.3922
step=16000 test_loss=0.0707 lr=0.0900 label_mean=0.3922
step=18000 test_loss=0.0691 lr=0.0900 label_mean=0.3922
step=20000 test_loss=0.0683 lr=0.0900 label_mean=0.3922
step=22000 test_loss=0.0665 lr=0.0900 label_mean=0.3922
step=24000 test_loss=0.0644 lr=0.0900 label_mean=0.3922
step=26000 test_loss=0.0619 lr=0.0900 label_mean=0.3922
step=28000 test_loss=0.0590 lr=0.0900 label_mean=0.3922
step=30000 test_loss=0.0555 lr=0.0900 label_mean=0.3922
step=32000 test_loss=0.0511 lr=0.0900 label_mean=0.3922
step=34000 test_loss=0.0465 lr=0.0900 label_mean=0.3922
step=36000 test_loss=0.0413 lr=0.0900 label_mean=0.3922
step=38000 test_loss=0.0367 lr=0.0900 label_mean=0.3922
step=40000 test_loss=0.0322 lr=0.0900 label_mean=0.3922
step=42000 test_loss=0.0283 lr=0.0900 label_mean=0.3922
step=44000 test_loss=0.0256 lr=0.0900 label_mean=0.3922
step=46000 test_loss=0.0237 lr=0.0900 label_mean=0.3922
step=48000 test_loss=0.0223 lr=0.0900 label_mean=0.3922
step=50000 test_loss=0.0215 lr=0.0900 label_mean=0.3922
step=52000 test_loss=0.0210 lr=0.0900 label_mean=0.3922
step=54000 test_loss=0.0207 lr=0.0900 label_mean=0.3922
step=56000 test_loss=0.0207 lr=0.0900 label_mean=0.3922
step=58000 test_loss=0.0203 lr=0.0900 label_mean=0.3922
step=60000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=62000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=64000 test_loss=0.0200 lr=0.0900 label_mean=0.3922
step=66000 test_loss=0.0200 lr=0.0900 label_mean=0.3922
step=68000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=70000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=72000 test_loss=0.0200 lr=0.0900 label_mean=0.3922
step=74000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=76000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=78000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=80000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=82000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=84000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=86000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=88000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=90000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=92000 test_loss=0.0202 lr=0.0900 label_mean=0.3922
step=94000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=96000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
step=98000 test_loss=0.0201 lr=0.0900 label_mean=0.3922
best precision = 0.020041
============= 0-layer's ================
weight =
[0.11919993667929912, -0.11230971916541774, -0.041353621543160975, 0.06166163262893083, -0.07293338833784946]
[-0.1336657072760772, -0.1071365000884227, -0.1836415389115575, -0.1833830795945298, -0.06908747233454084]
[0.17902819775539552, 0.01728136662689127, 0.038103096028957194, 0.21468754597018136, 0.013394837750428983]
[-0.1419291268465042, 0.1480126468020683, -0.23268588225496994, 0.025814255873293886, -0.276405036619337]
[-0.1732024152175099, 0.03386349553061654, 0.020974229710281546, -0.16025375438394118, -0.14298445655435507]
[-0.07354131676845078, -0.20694925772602835, -0.01153863542358512, 0.18453579205524717, -0.19559512193336362]
[0.02774654736181705, 0.257348065658914, 0.1807660227535751, 0.30163643245430743, 0.1637956151051957]
[-0.007407297500091754, -0.018225545352121487, -0.09270878057877709, 0.0836481141326543, -0.12036474461336213]
bias = 
 [0.0042185349089203135, 0.03869603687444764, -0.011734182577795807, 0.01368238490544803, 0.01650575691542247, 0.019052890373242624, -0.048466843560240364, 0.014753029486657295]
============= 1-layer's ================
weight =
[0.10371639190689641, -0.026959658414467232, 0.07240295653527566, -0.07948976465254136, -0.10046916651015603, 0.16827616556301359, 0.07215339719033483, 0.10591191214286093]
[0.019268074480825622, 0.28546994865124026, 0.018047903141863844, -0.08385772364735612, 0.09218938721776727, 0.0663911812821728, -0.2645111848263546, -0.0056669866668550605]
[0.08823630527594188, 0.05172746128722586, -0.011977219374174009, -0.1452348592803097, 0.03809152135533306, -0.13100210680459662, 0.16134560391359204, -0.04355355699522234]
[-0.06900788968758874, -0.012049278141294972, -0.11733334295115712, -0.016135160362270508, -0.06174397562892558, -0.02261015535789766, -0.014567487396006783, 0.08503984902064361]
[-0.13524985901662212, -0.170506561460729, 0.015948998436539935, -0.05800288464378994, -0.021652039243641428, -0.24932674670387614, 0.2499681593334484, -0.12170301936688899]
[0.18856528992188915, -0.13066694335532167, -0.10094507214883497, -0.019979355234077366, -0.02417614256665397, -0.08147942961964266, 0.07356547579779156, -0.17481295529539073]
[0.0592763297093407, -0.20790660036218817, 0.22419175615256642, -0.1493481453309948, -0.18809354333163486, -0.023342942760024455, 0.35051951217567806, -0.06379136940635244]
[-0.025044446282507755, -0.12156384604605396, -0.02297987035474797, -0.14607431934993861, -0.15818733722868636, -0.17770280678933892, 0.08291446975083387, 0.11977430383051246]
bias = 
 [0.003353840856625891, 0.04599609509905762, -0.011190093469313208, -0.013920118774816964, -0.04486018569419095, -0.020657121108150742, -0.047256962923671306, 0.00012727667356918378]
============= 2-layer's ================
weight =
[0.05991169610651531, -0.18327445303648912, 0.11248991010133659, -0.06966159202053798, 0.2958096258427969, 0.05427540595399094, 0.422897538571361, 0.22017781123144337]
bias = 
 [0.1415917527062357]
